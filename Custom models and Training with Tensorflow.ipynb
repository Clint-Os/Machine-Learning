{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rc('axes', labelsize = 14)\n",
    "mpl.rc('xtick', labelsize= 12)\n",
    "mpl.rc('ytick', labelsize = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors and Operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#matrix\n",
    "tf.constant([[1.,2.,3.], [4.,5.,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=42>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scalar\n",
    "tf.constant(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.constant([[1.,2.,3.], [4.,5.,6]])\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[2., 3.],\n",
       "       [5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
       "array([[2.],\n",
       "       [5.]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[..., 1, tf.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[11., 12., 13.],\n",
       "       [14., 15., 16.]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t +10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The @ operator was added in Python 3.5, for matrix multiplication: it is equivalent to calling the tf.matmul() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[14., 32.],\n",
       "       [32., 77.]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t @ tf.transpose(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From/To Numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([2.,4.,5.,])\n",
    "tf.constant(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.numpy()\n",
    "\n",
    "#from tf to np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(t)\n",
    "\n",
    "#from np to tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([ 4., 16., 25.])>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(a)\n",
    "\n",
    "#from tf to np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.square(t)\n",
    "\n",
    "\n",
    "#from np to tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customizing Models and Training Algorithms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Loss Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and prep the california housing dataset: load, split into a training set, a val set, and a test set, then scale it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1,1))\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " create a function that takes the labels and predictions as arguments, and use\n",
    "TensorFlow operations to compute every instance’s loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_fn(y_true, y_pred):\n",
    "    error =y_true - y_pred\n",
    "    is_small_error =  tf.abs(error)<1\n",
    "    squared_loss = tf.square(error)/2\n",
    "    linear_loss = tf.abs(error)-0.5\n",
    "    return tf.where(is_small_error,squared_loss,linear_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huber loss is not currently part of the official Keras API, but it is available in tf.keras (just use an instance of the keras.losses.Huber class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x350 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.figure(figsize = (8, 3.5))\n",
    "z = np.linspace(-4,4,200)\n",
    "plt.plot(z, huber_fn(0,z), 'b-',linewidth=2, label= \"huber($z$)\")\n",
    "plt.plot(z, z**2/2, \"b:\", linewidth=1, label = r\"$\\frac{1}{2}z^2$\")\n",
    "plt.plot([-1,1], [0, huber_fn(0., -1.)], \"r--\")\n",
    "plt.plot([1,1], [0, huber_fn(0., 1.)], \"r--\")\n",
    "plt.gca().axhline(y=0, color=\"k\")\n",
    "plt.gca().axvline(x=0, color=\"k\")\n",
    "plt.axis([-4,4,0,4])\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"$z$\")\n",
    "plt.legend(fontsize=14)\n",
    "plt.title(\"Huber loss\", fontsize=14)\n",
    "plt.show\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer= \"lecun_normal\", input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss= huber_fn, optimizer = \"nadam\", metrics = [\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.0808 - mae: 1.4924 - val_loss: 0.3462 - val_mae: 0.6356\n",
      "Epoch 2/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2189 - mae: 0.5155 - val_loss: 0.2854 - val_mae: 0.5701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x16d126d1a50>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train,epochs=2, validation_data = (X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving/Loading Models with Custom Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model_custom_loss.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_model_custom_loss.keras\", custom_objects = {\"huber_fn\": huber_fn})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.2118 - mae: 0.5042 - val_loss: 0.2395 - val_mae: 0.5212\n",
      "Epoch 2/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2064 - mae: 0.4963 - val_loss: 0.1961 - val_mae: 0.4758\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x16d15cf9210>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs = 2, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any error between -1 and 1 is considered “small”, but what if we want a different threshold? One solution is to create a function that creates a configured loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_huber(threshold=1.0):\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true-y_pred\n",
    "        is_small_error = tf.abs(error)<threshold\n",
    "        squared_loss = tf.square(error)/2\n",
    "        linear_loss = threshold * tf.abs(error)-threshold**2/2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    return huber_fn \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=create_huber(2.0), optimizer = \"nadam\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.2276 - mae: 0.4939 - val_loss: 0.2105 - val_mae: 0.4650\n",
      "Epoch 2/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2234 - mae: 0.4888 - val_loss: 0.1962 - val_mae: 0.4558\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x16d1729bb50>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2, validation_data = (X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model_custom_loss.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when you save the model, the threshold will not be saved. This means that you will have to specify the threshold value when loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model (\"my_model_custom_loss.keras\", custom_objects={\"huber_fn\":create_huber(2.0)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.2189 - mae: 0.4839 - val_loss: 0.1979 - val_mae: 0.4550\n",
      "Epoch 2/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2157 - mae: 0.4798 - val_loss: 0.2053 - val_mae: 0.4570\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x16d18eafb50>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve this by creating a subclass of the keras.losses.Loss class, and implement its get_config() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberLoss(keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold \n",
    "        super().__init__(**kwargs) \n",
    "    def call(self, y_true,y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) <self.threshold\n",
    "        squared_loss =tf.square(error)\n",
    "        linear_loss = self.threshold * tf.abs(error) - self.threshold**2/2\n",
    "        return tf.where(is_small_error,squared_loss, linear_loss)\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation = \"selu\", kernel_initializer=\"lecun_normal\",\n",
    "    input_shape=input_shape),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=HuberLoss(2.), optimizer = \"nadam\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.8853 - mae: 2.0864 - val_loss: 2.3117 - val_mae: 1.7136\n",
      "Epoch 2/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.1732 - mae: 1.6405 - val_loss: 1.6605 - val_mae: 1.3163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x16d1732bb50>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_custom_class.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"model_custom_class.keras\", custom_objects = {\"HuberLoss\": HuberLoss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.5722 - mae: 1.2692 - val_loss: 1.1532 - val_mae: 0.9928\n",
      "Epoch 2/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.1045 - mae: 0.9654 - val_loss: 0.8006 - val_mae: 0.7583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x16d12583b50>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs = 2, validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softplus(z): # return value is just tf.nn.softplus(z)\n",
    "    return tf.math.log(tf.exp(z) + 1.0)\n",
    "\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "def my_positive_weights(weights): # return value is just tf.nn.relu(weights)\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(1, activation = my_softplus,\n",
    "                            kernel_initializer = my_glorot_initializer,\n",
    "                            kernel_regularizer=my_l1_regularizer,\n",
    "                            kernel_constraint = my_positive_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a function has some hyperparameters that need to be saved along with the model,\n",
    "then just subclass the appropriate class, such as keras.regularizers.Regularizer,                 keras.constraints.Constraint, keras.initializers.Initializer or keras.layers.Layer (for any layer, including activation functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import Regularizer\n",
    "\n",
    "class MyL1Regularizer(Regularizer):\n",
    "    def __init__(self, l1=0.01):\n",
    "        self.l1 = l1\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.l1 * K.sum(K.abs(x))\n",
    "\n",
    "# Create an instance of your custom regularizer\n",
    "my_l1_regularizer= MyL1Regularizer(l1=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import Constraint\n",
    "\n",
    "class my_positive_weights(Constraint):\n",
    "    def __call__(self, weights):\n",
    "        return K.relu(weights)\n",
    "\n",
    "my_positive_weights = my_positive_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1, activation=my_softplus,\n",
    "                       kernel_regularizer=my_l1_regularizer,\n",
    "                       kernel_constraint=my_positive_weights,\n",
    "                       kernel_initializer=my_glorot_initializer),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer =\"lecun_normal\",\n",
    "    input_shape = input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss= \"mse\", optimizer = \"nadam\", metrics = [create_huber(2.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - huber_fn: 1.3559 - loss: 3.0876\n",
      "Epoch 2/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - huber_fn: 0.2631 - loss: 0.5375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x16d158fbb50>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs =2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE-:\n",
    "\n",
    "\n",
    "    1. The loss since the start of the epoch is the mean of all batch losses seen so far. Each batch loss is the sum of the weighted instance losses divided by the batch size (not the sum of weights, so the batch loss is not the weighted mean of the losses).\n",
    "\n",
    "    \n",
    "    2. The metric since the start of the epoch is equal to the sum of weighted instance losses divided by sum of all weights seen so far. In other words, it is the weighted mean of all the instance losses. Not the same thing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streaming Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.8>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = keras.metrics.Precision()\n",
    "precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, after the first batch, it returns the precision of 80%, then after the second batch it returns 50% (which is the overall precision so far, not the second batch’s precision). This is called a streaming metric (orstateful metric), as it is gradually updated, batch after batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At any point, we can call the result() method to get the current value of the metric.\n",
    "We can also look at its variables (tracking the number of true and false positives)\n",
    "using the variables attribute, and reset these variables using the reset_states() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasVariable shape=(1,), dtype=float32, path=precision/true_positives>,\n",
       " <KerasVariable shape=(1,), dtype=float32, path=precision/false_positives>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision.reset_states() is deprecated "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a strreaming metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberMetric(keras.metrics.Metric):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        super().__init__(**kwargs) # handles base args (e.g., dtype)\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        self.total.assign_add(tf.reduce_sum(metric))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "    def result(self):\n",
    "        return self.total / self.count\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• The constructor uses the add_weight() method to create the variables needed to\n",
    "keep track of the metric’s state over multiple batches, in this case the sum of all\n",
    "Huber losses (total) and the number of instances seen so far (count). You could\n",
    "just create variables manually if you preferred. Keras tracks any tf.Variable that\n",
    "is set as an attribute (and more generally, any “trackable” object, such as layers or\n",
    "models).\n",
    "\n",
    "\n",
    "• The update_state() method is called when you use an instance of this class as a\n",
    "function (as we did with the Precision object). It updates the variables given the\n",
    "labels and predictions for one batch (and sample weights, but in this case we just\n",
    "ignore them).\n",
    "\n",
    "\n",
    "• The result() method computes and returns the final result, in this case just the\n",
    "mean Huber metric over all instances. When you use the metric as a function, the\n",
    "update_state() method gets called first, then the result() method is called,\n",
    "and its output is returned.\n",
    "\n",
    "\n",
    "• We also implement the get_config() method to ensure the threshold gets\n",
    "saved along with the model.\n",
    "\n",
    "• The default implementation of the reset_states() method just resets all vari‐\n",
    "ables to 0.0 (but you can override it if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberMetric(keras.metrics.Mean):\n",
    "    def __init__(self,threshold, name = 'huber_metric', dtype=tf.float32):\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "        super().__init__(name=name, dtype=dtype)\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        super(HuberMetric, self).update_state(metric, sample_weight)\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class handles shape better and supports sample weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation = \"selu\", kernel_initializer = \"lecun_normal\", input_shape = input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = keras.losses.Huber(2.0), optimizer = \"nadam\", weighted_metrics = [HuberMetric(2.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - huber_metric: 1.6699 - loss: 0.8311\n",
      "Epoch 2/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - huber_metric: 0.2629 - loss: 0.1307\n"
     ]
    }
   ],
   "source": [
    "sample_weight = np.random.rand(len(y_train))\n",
    "history = model.fit(X_train_scaled.astype(np.float32), y_train.astype(np.float32), epochs=2, sample_weight= sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.44582274556159973, 0.44582282034206727)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history[\"loss\"][0], history.history[\"huber_metric\"][0] * sample_weight.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to create a custom layer without any weights, the simplest option is to write a function and wrap it in a keras.layers.Lambda layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "exponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tf.constant([-1.0, 0.0, 1.0], dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.36787945, 1.        , 2.7182817 ], dtype=float32)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exponential_layer(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " exponential layer is sometimes used in the output layer of a regression model when the values to predict have very different scales (e.g., 0.001, 10., 1000.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.6677 - val_loss: 0.4546\n",
      "Epoch 2/5\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4934 - val_loss: 0.3889\n",
      "Epoch 3/5\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4355 - val_loss: 0.3702\n",
      "Epoch 4/5\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4185 - val_loss: 0.3590\n",
      "Epoch 5/5\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.4153 - val_loss: 0.3529\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3708721101284027"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation = \"relu\", input_shape = input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "    exponential_layer\n",
    "  \n",
    " ])\n",
    "\n",
    "model.compile(loss= \"mse\", optimizer=\"sgd\")\n",
    "model.fit(X_train_scaled, y_train, epochs=5, validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "class MyDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super(MyDense, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "    \n",
    "    def build(self, batch_input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            name=\"kernel\", \n",
    "            shape=(batch_input_shape[-1], self.units), \n",
    "            initializer=\"glorot_normal\",\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"bias\", \n",
    "            shape=(self.units,), \n",
    "            initializer=\"zeros\", \n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        super(MyDense, self).build(batch_input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z = tf.matmul(inputs, self.kernel) + self.bias\n",
    "        return self.activation(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_20772\\3691450218.py:6: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super(MyDense, self).__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    MyDense(30, activation=\"relu\", input_shape=input_shape),\n",
    "    MyDense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:25: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 3.8616 - val_loss: 1.4324\n",
      "Epoch 2/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.7134 - val_loss: 0.5722\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 942us/step - loss: 0.5247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5197387933731079"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = (8,)  # Adjusted input shape to match the data\n",
    "\n",
    "# Define the model with the correct input shape\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=input_shape),  # Specify adjusted input shape\n",
    "    MyDense(30, activation='relu'),\n",
    "    MyDense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"nadam\", loss=\"mse\")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=2, validation_data=(X_valid_scaled, y_valid))\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_with_custom_layer.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a layer with multiple inputs (e.g., Concatenate), the argument to the call() method should be a tuple containing all the inputs, and similarly the argument to the compute_output_shape() method should be a tuple containing each input’s batch shape. To create a layer with multiple outputs, the call() method should return the list of outputs, and the compute_output_shape() should return the list of batch output shapes (one per output)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multilayer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMultiLayer(keras.layers.Layer):\n",
    "    def call(self, X):\n",
    "        X1, X2 = X\n",
    "        print(\"X1.shape: \", X1.shape, \"X2.shape: \",X2.shape) #debugging custom layer \n",
    "        return X1 + X2, X1 * X2\n",
    "    \n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        batch_input_shape1, batch_input_shape2 = batch_input_shape\n",
    "        return [batch_input_shape1, batch_input_shape2]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can call the custom layer using functional API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1 = keras.layers.Input(shape=[2])\n",
    "inputs2 = keras.layers.Input(shape=[2])\n",
    "outputs1, outputs2 = MyMultiLayer()((inputs1, inputs2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your layer needs to have a different behavior during training and during testing\n",
    "(e.g., if it uses Dropout or BatchNormalization layers), then you must add a training argument to the call() method and use this argument to decide what to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGausianNoise(keras.layers.Layer):\n",
    "    def __init__(self, stddev,  **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def call(self, X, training=None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(X), stddev = self.stddev)\n",
    "            return X + noise\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    AddGausianNoise(stddev=1.0),\n",
    "    keras.layers.Dense(30, activation = \"selu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 3.6923 - val_loss: 0.7833\n",
      "Epoch 2/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0643 - val_loss: 0.8858\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7564\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7579102516174316"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss= \"mse\", optimizer = \"nadam\")\n",
    "model.fit(X_train_scaled, y_train, epochs =2, validation_data=(X_valid_scaled,y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_scaled = X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "class ResidualBlock(keras.layers.Layer):\n",
    "    def __init__(self, units, activation='relu', **kwargs):\n",
    "        super(ResidualBlock, self).__init__(**kwargs)\n",
    "        self.hidden = keras.layers.Dense(units, activation=activation)\n",
    "        self.batch_norm = keras.layers.BatchNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden(inputs)\n",
    "        Z = self.batch_norm(Z)\n",
    "        return Z + inputs\n",
    "\n",
    "class ResidualRegressor(keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super(ResidualRegressor, self).__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(30, activation=\"relu\")\n",
    "        self.block1 = ResidualBlock(30)\n",
    "        self.block2 = ResidualBlock(30)\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs)\n",
    "        Z = self.block1(Z)\n",
    "        Z = self.block2(Z)\n",
    "        return self.out(Z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 1.1397\n",
      "Epoch 2/5\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4021\n",
      "Epoch 3/5\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3781\n",
      "Epoch 4/5\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3743\n",
      "Epoch 5/5\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3694\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3701\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "model = ResidualRegressor(1)\n",
    "model.compile(loss=\"mse\", optimizer =\"nadam\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs = 5)\n",
    "score = model.evaluate(X_test_scaled, y_test)\n",
    "y_pred = model.predict(X_new_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the subclassing API to define the model itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "block1 = ResidualBlock(30)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    block1, block1, block1, block1,\n",
    "    ResidualBlock(30),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 5.6914\n",
      "Epoch 2/5\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.8574\n",
      "Epoch 3/5\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6439\n",
      "Epoch 4/5\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5553\n",
      "Epoch 5/5\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5086\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6597\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = \"mse\", optimizer =\"nadam\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs = 5)\n",
    "score = model.evaluate(X_test_scaled, y_test)\n",
    "y_pred = model.predict(X_new_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Losses and Metrics Based on Model Internals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconstructingRegressor(keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(30, activation=\"selu\",\n",
    "                                          kernel_initializer=\"lecun_normal\")\n",
    "                       for _ in range(5)]\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "        self.reconstruction_mean = keras.metrics.Mean(name=\"reconstruction_error\")\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        n_inputs = batch_input_shape[-1]\n",
    "        self.reconstruct = keras.layers.Dense(n_inputs)\n",
    "        #super().build(batch_input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        reconstruction = self.reconstruct(Z)\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "        self.add_loss(0.05 * recon_loss)\n",
    "        if training:\n",
    "            result = self.reconstruction_mean(recon_loss)\n",
    "        return self.out(Z)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the aboce code, we create keras.metrics.Mean() in the constructor and use it in the call() method to track the mean reconstruction loss. We add a training argument to the call() method and if training is True, then we update the reconstruction_mean and we call self.add_metric() to ensure it's displayed properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 1.4479 - reconstruction_error: 0.9713\n",
      "Epoch 2/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4494 - reconstruction_error: 0.3839\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "model = ReconstructingRegressor(1)\n",
    "model.compile(loss=\"mse\", optimizer= \"nadam\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs = 2)\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing Gradients with Autodiff "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define two variables w1 and w2, then we create a tf.GradientTape context\n",
    "that will automatically record every operation that involves a variable, and finally we\n",
    "ask this tape to compute the gradients of the result z with regards to both variables\n",
    "[w1, w2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w1, w2):\n",
    "    return 3*w1 ** 2 + 2*w1 * w2\n",
    "\n",
    "#toy function for illustration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.000003007075065"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1, w2 = 5, 3\n",
    "eps = 1e-6\n",
    "(f(w1 + eps, w2) - f(w1, w2)) / eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, w2 = tf.Variable(5.0), tf.Variable(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "gradients = tape.gradient(z, [w1, w2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "dz_dw1 = tape.gradient(z, w1)\n",
    "try:\n",
    "    dz_dw2 = tape.gradient(z, w2)\n",
    "except RuntimeError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to call gradient() more than once, you must make the tape persistent,\n",
    "and delete it when you are done with it to free resources.\n",
    "\n",
    "By default, the tape will only track operations involving variables, so if you try to\n",
    "compute the gradient of z with regards to anything else than a variable, the result will\n",
    "be None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "dz_dw1 = tape.gradient(z, w1)\n",
    "dz_dw2 = tape.gradient(z, w2)  # can only calculate once\n",
    "\n",
    "del tape  #worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dz_dw1, dz_dw2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 , c2 = tf.constant(5.), tf.constant(3.) #non variables\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(c1, c2)\n",
    "\n",
    "gradients  = tape.gradient(z, [c1, c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients #only tracks variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, you can force the tape to watch any tensors you like, to record every opera‐\n",
    "tion that involves them. You can then compute gradients with regards to these ten‐\n",
    "sors, as if they were variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(c1)\n",
    "    tape.watch(c2)\n",
    "    z = f(c1, c2)\n",
    "\n",
    "gradients = tape.gradient(z, [c1, c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients  #it works! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above can be useful in some cases, for example if you want to implement a regularization loss that penalizes activations that vary a lot when the inputs vary little: the loss will be based on the gradient of the activations with regards to the inputs. Since the inputs are not variables, you would need to tell the tape to watch them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is actually possible to compute second order partial derivatives (the Hessians, i.e., the partial derivatives of the partial derivatives)! To do this, we need to record the operations that are performed when computing the first-order partial derivatives (the Jacobians): this requires a second tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape(persistent = True) as hessian_tape:\n",
    "    with tf.GradientTape() as jacobian_tape:\n",
    "        z = f(w1, w2)\n",
    "    jacobians = jacobian_tape.gradient(z, [w1, w2])\n",
    "    hessians = [hessian_tape.gradient(jacobian, [w1, w2]) for jacobian in jacobians]\n",
    "    del hessian_tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<tf.Tensor: shape=(), dtype=float32, numpy=6.0>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=2.0>],\n",
       " [<tf.Tensor: shape=(), dtype=float32, numpy=2.0>, None]]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hessians "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some rare cases you may want to stop gradients from backpropagating through\n",
    "some part of your neural network. To do this, you must use the tf.stop_gradient()\n",
    "function: it just returns its inputs during the forward pass (like tf.identity()), but\n",
    "it does not let gradients through during backpropagation (it acts like a constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=30.0>, None]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(w1, w2):\n",
    "    return 3*w1**2 + tf.stop_gradient(2*w1*w2)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)  #same result as without stop_gradient()\n",
    "\n",
    "tape.gradient(z, [w1, w2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may occasionally run into some numerical issues when computing gradi‐\n",
    "ents. For example, if you compute the gradients of the my_softplus() function for\n",
    "large inputs, the result will be NaN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=nan>]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(100.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_softplus(x)\n",
    "\n",
    "tape.gradient(z, [x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because computing the gradients of this function using autodiff leads to some\n",
    "numerical difficulties: due to floating point precision errors, autodiff ends up computing infinity divided by infinity (which returns NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=30.0>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Analytically find the stable value\n",
    "tf.math.log(tf.exp(tf.constant(30., dtype = tf.float32)) + 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1,), dtype=float32, numpy=array([nan], dtype=float32)>]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable([100.])\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_softplus(x)\n",
    "\n",
    "tape.gradient(z, [x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decorate it with @tf.custom_gradient, and make it return both its normal output and the function\n",
    "that computes the derivatives (note that it will receive as input the gradients that were\n",
    "backpropagated so far, down to the softplus function, and according to the chain rule\n",
    "we should multiply them with this function’s gradients):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def my_better_softplus(z):\n",
    "    exp = tf.exp(z)\n",
    "    def my_softplus_gradients(grad):\n",
    "        return grad / (1 + 1 / exp)\n",
    "    return tf.math.log(exp + 1), my_softplus_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_better_softplus(z):\n",
    "    return tf.where(z > 0, z, tf.math.log(z + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable([1000.])\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_better_softplus(x)\n",
    "\n",
    "tape.gradient(z, [x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Training Loops "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some rare cases, the fit() method may not be flexible enough for what you need\n",
    "to do, for exmaple when using two different optimizers, one for a wide path and the other for a deep path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first build a simple model \n",
    "\n",
    "l2_reg = keras.regularizers.l2(0.05)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=l2_reg),\n",
    "    keras.layers.Dense(1, kernel_regularizer=l2_reg)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a tiny function that will randomly sample a batch of instances from the training set \n",
    "\n",
    "def random_batch (X, y, batch_size=32):\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    return X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function also that displays the training status including number of steps, mean loss since start of the epoch etc\n",
    "\n",
    "def print_status_bar(iteration, total, loss, metrics=None):\n",
    "    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result()) #formats a float with 4 digits after the decimal \n",
    "                         for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{}/{} - \".format(iteration, total) + metrics, end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the hyperparameters\n",
    "\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n",
    "loss_fn = keras.losses.MeanSquaredError()\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics = [keras.metrics.MeanAbsoluteError()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "11610/11610 - mean: 1.1529 - mean_absolute_error: 0.5513\n",
      "Epoch 2/5\n",
      "11610/11610 - mean: 0.6490 - mean_absolute_error: 0.5183\n",
      "Epoch 3/5\n",
      "11610/11610 - mean: 0.6445 - mean_absolute_error: 0.5203\n",
      "Epoch 4/5\n",
      "11610/11610 - mean: 0.6566 - mean_absolute_error: 0.5270\n",
      "Epoch 5/5\n",
      "11610/11610 - mean: 0.6528 - mean_absolute_error: 0.5252\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs +1):\n",
    "    print(\"Epoch {}/{}\".format(epoch, n_epochs)) #create two nested loops: one for the epochs, the other for the batches within an epoch.\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(X_train_scaled, y_train) #sample random batch from the training set\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch) #make a prediction for one batch, using model as a function \n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred)) #calculate the mean loss for the batch\n",
    "            loss = tf.add_n([main_loss] + model.losses) #add the regularization losses from the model\n",
    "        gradients = tape.gradient(loss, model.trainable_variables) #compute the gradient of the loss with regard to trainable variable only\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables)) #apply them to the optimizer to eperforma gadient descent step\n",
    "        for variable in model.variables:\n",
    "            if variable.constraint is not None:\n",
    "                variable.assign(variable.constraint(variable))\n",
    "        mean_loss(loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n",
    "    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow Functions and Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with a trivial function for demonstration purposes:\n",
    "\n",
    "def cube(x):\n",
    "    return x**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can call using int, float or tensor\n",
    "cube(tf.constant(2.0)) #OR cube(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.polymorphic_function.polymorphic_function.Function at 0x16d2e1aac50>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use tf.function()  to convert python function to tensorflow function:\n",
    "\n",
    "tf_cube = tf.function(cube)\n",
    "tf_cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=8>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This TF Function can then be used exactly like the original Python function, and it\n",
    "# will return the same result (but as tensors):\n",
    "\n",
    "tf_cube(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, tf.function() analyzed the computations performed by the cube()\n",
    "function and generated an equivalent computation graph!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use tf.function as a decorator, which is more common:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function \n",
    "def tf_cube(X):\n",
    "    return X **3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring Function Definitions and Graphs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_function = tf_cube.get_concrete_function(tf.constant(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_function(tf.constant(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_function is tf_cube.get_concrete_function(tf.constant(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.framework.func_graph.FuncGraph at 0x16d25a48ec0>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_function.graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'X' type=Placeholder>,\n",
       " <tf.Operation 'pow/y' type=Const>,\n",
       " <tf.Operation 'pow' type=Pow>,\n",
       " <tf.Operation 'Identity' type=Identity>]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ops = concrete_function.graph.get_operations()\n",
    "ops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'X:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'pow/y:0' shape=() dtype=float32>]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow_op = ops[2]\n",
    "list(pow_op.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'pow:0' shape=() dtype=float32>]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow_op.outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"__inference_tf_cube_942393\"\n",
       "input_arg {\n",
       "  name: \"x\"\n",
       "  type: DT_FLOAT\n",
       "}\n",
       "output_arg {\n",
       "  name: \"identity\"\n",
       "  type: DT_FLOAT\n",
       "}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_function.function_def.signature "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Autograph to Capture Control Flow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A static for loop using range():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def add_10(x):\n",
    "    for i in range(10):\n",
    "        x += 1 \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=15>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_10(tf.constant(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'x' type=Placeholder>,\n",
       " <tf.Operation 'add/y' type=Const>,\n",
       " <tf.Operation 'add' type=AddV2>,\n",
       " <tf.Operation 'add_1/y' type=Const>,\n",
       " <tf.Operation 'add_1' type=AddV2>,\n",
       " <tf.Operation 'add_2/y' type=Const>,\n",
       " <tf.Operation 'add_2' type=AddV2>,\n",
       " <tf.Operation 'add_3/y' type=Const>,\n",
       " <tf.Operation 'add_3' type=AddV2>,\n",
       " <tf.Operation 'add_4/y' type=Const>,\n",
       " <tf.Operation 'add_4' type=AddV2>,\n",
       " <tf.Operation 'add_5/y' type=Const>,\n",
       " <tf.Operation 'add_5' type=AddV2>,\n",
       " <tf.Operation 'add_6/y' type=Const>,\n",
       " <tf.Operation 'add_6' type=AddV2>,\n",
       " <tf.Operation 'add_7/y' type=Const>,\n",
       " <tf.Operation 'add_7' type=AddV2>,\n",
       " <tf.Operation 'add_8/y' type=Const>,\n",
       " <tf.Operation 'add_8' type=AddV2>,\n",
       " <tf.Operation 'add_9/y' type=Const>,\n",
       " <tf.Operation 'add_9' type=AddV2>,\n",
       " <tf.Operation 'Identity' type=Identity>]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_10.get_concrete_function(tf.constant(5)).graph.get_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dynamic loop using tf.while_loop():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def add_10(x):\n",
    "    condition = lambda i, x: tf.less(i, 10)\n",
    "    body = lambda i, x: (tf.add(i, 1), tf.add(x, 1))\n",
    "    final_i, final_x = tf.while_loop(condition, body, [tf.constant(0), x])\n",
    "    return final_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=15>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_10(tf.constant(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'x' type=Placeholder>,\n",
       " <tf.Operation 'Const' type=Const>,\n",
       " <tf.Operation 'while/maximum_iterations' type=Const>,\n",
       " <tf.Operation 'while/loop_counter' type=Const>,\n",
       " <tf.Operation 'while' type=StatelessWhile>,\n",
       " <tf.Operation 'Identity' type=Identity>]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_10.get_concrete_function(tf.constant(5)).graph.get_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dynamic for loop captured by autograph: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function \n",
    "def add_10(x):\n",
    "    for i in tf.range(10):\n",
    "        x += 1  \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'x' type=Placeholder>,\n",
       " <tf.Operation 'range/start' type=Const>,\n",
       " <tf.Operation 'range/limit' type=Const>,\n",
       " <tf.Operation 'range/delta' type=Const>,\n",
       " <tf.Operation 'range' type=Range>,\n",
       " <tf.Operation 'sub' type=Sub>,\n",
       " <tf.Operation 'floordiv' type=FloorDiv>,\n",
       " <tf.Operation 'mod' type=FloorMod>,\n",
       " <tf.Operation 'zeros_like' type=Const>,\n",
       " <tf.Operation 'NotEqual' type=NotEqual>,\n",
       " <tf.Operation 'Cast' type=Cast>,\n",
       " <tf.Operation 'add' type=AddV2>,\n",
       " <tf.Operation 'zeros_like_1' type=Const>,\n",
       " <tf.Operation 'Maximum' type=Maximum>,\n",
       " <tf.Operation 'while/maximum_iterations' type=Const>,\n",
       " <tf.Operation 'while/loop_counter' type=Const>,\n",
       " <tf.Operation 'while' type=StatelessWhile>,\n",
       " <tf.Operation 'Identity' type=Identity>]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_10.get_concrete_function(tf.constant(0)).graph.get_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
